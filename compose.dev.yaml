# Remove version for modern Docker Compose

services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${DB_NAME:-tailmatch}
      POSTGRES_USER: ${DB_USER:-tailmatch_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-secure_password_change_this}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    ports:
      - "${DB_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-tailmatch_user} -d ${DB_NAME:-tailmatch}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - tail-match-network

  # Next.js Web Application
  web:
    build:
      context: .
      dockerfile: Dockerfile
      target: ${TARGET:-development}
    restart: unless-stopped
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - DATABASE_URL=postgresql://${DB_USER:-tailmatch_user}:${DB_PASSWORD:-secure_password_change_this}@db:5432/${DB_NAME:-tailmatch}
      - NEXT_PORT=${NEXT_PORT:-3000}
    ports:
      - "${NEXT_PORT:-3000}:3000"
    volumes:
      # Development mode: mount source for hot reload
      - .:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      db:
        condition: service_healthy
    networks:
      - tail-match-network
    command: ${WEB_COMMAND:-npm run dev}

  # Python Scraper Service
  scraper:
    build:
      context: ./scraper
      dockerfile: Dockerfile
    restart: unless-stopped
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-tailmatch}
      - DB_USER=${DB_USER:-tailmatch_user}
      - DB_PASSWORD=${DB_PASSWORD:-secure_password_change_this}
      - SCRAPING_ENABLED=${SCRAPING_ENABLED:-true}
      - SCRAPING_INTERVAL_SECONDS=${SCRAPING_INTERVAL_SECONDS:-4}
      - MAX_CONCURRENT_SCRAPES=${MAX_CONCURRENT_SCRAPES:-3}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    volumes:
      # Mount scraper source for development
      - ./scraper:/app
      - scraper_logs:/app/logs
    depends_on:
      db:
        condition: service_healthy
    networks:
      - tail-match-network
    # Run scraper in daemon mode by default
    command: python main.py --daemon

  # Redis for caching (optional, for future optimization)
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - tail-match-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

# Named volumes for data persistence
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  scraper_logs:
    driver: local

# Network for service communication
networks:
  tail-match-network:
    driver: bridge