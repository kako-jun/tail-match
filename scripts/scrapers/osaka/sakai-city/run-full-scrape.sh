#!/bin/bash

# ================================================================
# å ºå¸‚å‹•ç‰©æŒ‡å°ã‚»ãƒ³ã‚¿ãƒ¼ å®Œå…¨è‡ªå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ================================================================
#
# ä½¿ç”¨æ–¹æ³•:
#   ./scripts/scrapers/osaka/sakai-city/run-full-scrape.sh
#
# å®Ÿè¡Œå†…å®¹:
#   1. ç´¢å¼•ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸URLã‚’å–å¾—
#   2. å„ãƒšãƒ¼ã‚¸ã®HTMLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
#   3. å…¨ç”»åƒURLã‚’æŠ½å‡º
#   4. ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
#   5. YAMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆ
#   6. ï¼ˆæ‰‹å‹•ï¼‰Claude Vision APIã§ç”»åƒæƒ…å ±ã‚’æŠ½å‡º
#
# ================================================================

set -e  # ã‚¨ãƒ©ãƒ¼ã§åœæ­¢

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../../../.." && pwd)"
cd "$PROJECT_ROOT"

echo "============================================================"
echo "ğŸ± å ºå¸‚å‹•ç‰©æŒ‡å°ã‚»ãƒ³ã‚¿ãƒ¼ - å®Œå…¨è‡ªå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"
echo "============================================================"
echo ""

# ================================================================
# Step 1: ç´¢å¼•ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸URLã‚’æŠ½å‡º
# ================================================================

echo "[Step 1] ç´¢å¼•ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸URLã‚’å–å¾—..."

INDEX_URL="https://www.city.sakai.lg.jp/kurashi/dobutsu/dogcat/inunekojoto/index.html"
PAGE_URLS=$(curl -k -s "$INDEX_URL" | \
  grep -o 'href="[^"]*\(cats\|centerdogs\)[^"]*\.html"' | \
  sed 's/href="//;s/"$//' | \
  sort -u)

echo "è¦‹ã¤ã‹ã£ãŸãƒšãƒ¼ã‚¸:"
echo "$PAGE_URLS" | while read -r url; do
  echo "  - https://www.city.sakai.lg.jp${url}"
done
echo ""

# ================================================================
# Step 2: å„ãƒšãƒ¼ã‚¸ã®HTMLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ================================================================

echo "[Step 2] å„ãƒšãƒ¼ã‚¸ã®HTMLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­..."

HTML_DIR="data/html/osaka/sakai-city"
mkdir -p "$HTML_DIR"

page_num=1
echo "$PAGE_URLS" | while read -r rel_url; do
  full_url="https://www.city.sakai.lg.jp${rel_url}"
  timestamp=$(date -u +"%Y%m%d_%H%M%S")

  # ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š
  if echo "$rel_url" | grep -q "cats1"; then
    filename="${timestamp}_cats1.html"
  elif echo "$rel_url" | grep -q "cats2"; then
    filename="${timestamp}_cats2.html"
  elif echo "$rel_url" | grep -q "cats3"; then
    filename="${timestamp}_cats3.html"
  elif echo "$rel_url" | grep -q "centerdogs"; then
    filename="${timestamp}_dogs.html"
  else
    filename="${timestamp}_page${page_num}.html"
  fi

  echo "  [$page_num] $full_url"

  # Playwrightã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
  node -e "
    import('playwright').then(async ({ chromium }) => {
      const browser = await chromium.launch({ headless: true });
      const context = await browser.newContext({
        userAgent: 'TailMatch/1.0 (+https://tail-match.llll-ll.com) - ä¿è­·çŒ«æƒ…å ±åé›†Bot'
      });
      const page = await context.newPage();
      await page.goto('${full_url}', {
        waitUntil: 'networkidle',
        timeout: 30000
      });
      await page.waitForTimeout(3000);
      const html = await page.content();

      const fs = await import('fs');
      const path = await import('path');
      fs.writeFileSync('${HTML_DIR}/${filename}', html, 'utf-8');
      console.log('      âœ… ä¿å­˜: ${filename} (' + html.length + ' æ–‡å­—)');

      await browser.close();
    });
  "

  page_num=$((page_num + 1))
  sleep 2  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
done

echo ""

# ================================================================
# Step 3: å…¨ç”»åƒURLã‚’æŠ½å‡º
# ================================================================

echo "[Step 3] å…¨ç”»åƒURLã‚’æŠ½å‡ºä¸­..."

IMAGE_URLS=$(grep -oh '\(cats[0-9]\|centerdogs\)\.images/[^"]*\.png' "$HTML_DIR"/*.html 2>/dev/null | sort -u || true)

if [ -z "$IMAGE_URLS" ]; then
  echo "  âš ï¸  ç”»åƒURLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
  exit 1
fi

image_count=$(echo "$IMAGE_URLS" | wc -l)
echo "  è¦‹ã¤ã‹ã£ãŸç”»åƒ: ${image_count}æš"
echo "$IMAGE_URLS" | head -5
if [ "$image_count" -gt 5 ]; then
  echo "  ... (æ®‹ã‚Š $((image_count - 5))æš)"
fi
echo ""

# ================================================================
# Step 4: ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# ================================================================

echo "[Step 4] ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."

IMAGE_DIR="data/images/osaka/sakai-city"
mkdir -p "$IMAGE_DIR"

downloaded=0
echo "$IMAGE_URLS" | while read -r img_path; do
  if [ -n "$img_path" ]; then
    filename=$(basename "$img_path")
    full_url="https://www.city.sakai.lg.jp/kurashi/dobutsu/dogcat/inunekojoto/${img_path}"

    curl -k -s -o "${IMAGE_DIR}/${filename}" "$full_url"

    if [ -f "${IMAGE_DIR}/${filename}" ]; then
      size=$(du -h "${IMAGE_DIR}/${filename}" | cut -f1)
      echo "    âœ… ${filename} (${size})"
      downloaded=$((downloaded + 1))
    else
      echo "    âŒ ${filename} - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—"
    fi

    sleep 1  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
  fi
done

echo "  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: ${downloaded}/${image_count}æš"
echo ""

# ================================================================
# Step 5: YAMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
# ================================================================

echo "[Step 5] YAMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆä¸­..."

node "$SCRIPT_DIR/extract-from-images.js"

echo ""

# ================================================================
# å®Œäº†
# ================================================================

echo "============================================================"
echo "âœ… è‡ªå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†"
echo "============================================================"
echo ""
echo "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:"
echo "  1. data/images/osaka/sakai-city/ ã®ç”»åƒã‚’ç¢ºèª"
echo "  2. Claude ã«ç”»åƒã‚’è¦‹ã›ã¦æƒ…å ±ã‚’æŠ½å‡º"
echo "  3. YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"
echo "  4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŠ•å…¥:"
echo "     node scripts/yaml-to-db.js"
echo ""
