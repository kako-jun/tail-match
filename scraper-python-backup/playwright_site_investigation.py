#!/usr/bin/env python3
"""
Playwright Site Investigation
å®Ÿéš›ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚µã‚¤ãƒˆæ§‹é€ ã‚’èª¿æŸ»
"""

import asyncio
import json
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# èª¿æŸ»å¯¾è±¡ã‚µã‚¤ãƒˆ
SITES_TO_INVESTIGATE = [
    {
        'name': 'æ±äº¬éƒ½å‹•ç‰©æ„›è­·ç›¸è«‡ã‚»ãƒ³ã‚¿ãƒ¼',
        'url': 'https://www.fukushihoken.metro.tokyo.lg.jp/douso/jouto/cat_jouto.html',
        'prefecture': 'æ±äº¬éƒ½',
        'region': 'kanto'
    },
    {
        'name': 'ç¥å¥ˆå·çœŒå‹•ç‰©ä¿è­·ã‚»ãƒ³ã‚¿ãƒ¼',
        'url': 'https://www.pref.kanagawa.jp/docs/v7d/cnt/f80192/p1190156.html',
        'prefecture': 'ç¥å¥ˆå·çœŒ',
        'region': 'kanto'
    },
    {
        'name': 'åƒè‘‰çœŒå‹•ç‰©æ„›è­·ã‚»ãƒ³ã‚¿ãƒ¼',
        'url': 'https://www.pref.chiba.lg.jp/aigo/jouhou/jouhou-top.html',
        'prefecture': 'åƒè‘‰çœŒ',
        'region': 'kanto'
    },
    {
        'name': 'å¤§é˜ªåºœå‹•ç‰©æ„›è­·ç®¡ç†ã‚»ãƒ³ã‚¿ãƒ¼',
        'url': 'http://www.pref.osaka.lg.jp/doaicenter/joutojouhou/index.html',
        'prefecture': 'å¤§é˜ªåºœ',
        'region': 'kinki'
    },
    {
        'name': 'æ„›çŸ¥çœŒå‹•ç‰©æ„›è­·ã‚»ãƒ³ã‚¿ãƒ¼',
        'url': 'https://www.pref.aichi.jp/soshiki/dobutsu/neko-joto.html',
        'prefecture': 'æ„›çŸ¥çœŒ',
        'region': 'chubu'
    }
]

async def analyze_page_structure(page, url):
    """ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’åˆ†æ"""
    try:
        # ãƒšãƒ¼ã‚¸ã‚’é–‹ã
        print(f"  ğŸ“¡ Loading {url}...")
        await page.goto(url, wait_until='networkidle', timeout=30000)

        # ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        await page.wait_for_timeout(2000)

        # HTMLã‚’å–å¾—
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')

        analysis = {
            'html_length': len(html),
            'title': await page.title(),
            'url': page.url,  # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¾Œã®URL
            'screenshot_path': None,
            'structure': {}
        }

        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±
        screenshot_filename = f"screenshot_{url.split('//')[1].replace('/', '_').replace('.', '_')[:50]}.png"
        try:
            await page.screenshot(path=screenshot_filename, full_page=False)
            analysis['screenshot_path'] = screenshot_filename
            print(f"  ğŸ“¸ Screenshot saved: {screenshot_filename}")
        except Exception as e:
            print(f"  âš ï¸  Screenshot failed: {e}")

        # æ§‹é€ åˆ†æ
        print(f"  ğŸ” Analyzing structure...")

        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
        tables = soup.find_all('table')
        analysis['structure']['tables'] = []
        for i, table in enumerate(tables[:5]):
            rows = table.find_all('tr')
            table_data = {
                'index': i,
                'rows': len(rows),
                'has_thead': bool(table.find('thead')),
                'classes': table.get('class', []),
                'sample_text': table.get_text(strip=True)[:200]
            }
            analysis['structure']['tables'].append(table_data)

        # çŒ«é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
        cat_keywords = ['çŒ«', 'ãƒã‚³', 'ã­ã“']
        keyword_elements = []

        for keyword in cat_keywords:
            elements = soup.find_all(text=lambda t: t and keyword in t)
            if elements:
                keyword_elements.append({
                    'keyword': keyword,
                    'count': len(elements),
                    'sample': [elem.strip()[:100] for elem in elements[:3] if elem.strip()]
                })

        analysis['structure']['cat_keywords'] = keyword_elements

        # ç”»åƒã‚’æ¢ã™
        images = soup.find_all('img')
        analysis['structure']['images'] = {
            'total': len(images),
            'alt_texts': [img.get('alt', '')[:50] for img in images[:10] if img.get('alt')]
        }

        # ãƒªã‚¹ãƒˆã‚’æ¢ã™
        lists = soup.find_all(['ul', 'ol'])
        analysis['structure']['lists'] = {
            'total': len(lists),
            'large_lists': sum(1 for lst in lists if len(lst.find_all('li')) > 3)
        }

        # å¯èƒ½æ€§ã®ã‚ã‚‹ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
        possible_containers = []
        selectors_to_check = [
            ('.animal-list', 'animal-list class'),
            ('.cat-list', 'cat-list class'),
            ('.jouto-list', 'jouto-list class'),
            ('.data_box', 'data_box class'),
            ('[class*="animal"]', 'animal in class'),
            ('[class*="cat"]', 'cat in class'),
            ('[class*="neko"]', 'neko in class')
        ]

        for selector, desc in selectors_to_check:
            elements = soup.select(selector)
            if elements:
                possible_containers.append({
                    'selector': selector,
                    'description': desc,
                    'count': len(elements),
                    'sample_text': elements[0].get_text(strip=True)[:100] if elements else ''
                })

        analysis['structure']['possible_containers'] = possible_containers

        # JavaScriptãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¤œå‡º
        frameworks = []
        if 'react' in html.lower() or 'data-reactroot' in html:
            frameworks.append('React')
        if 'vue' in html.lower() or 'v-app' in html:
            frameworks.append('Vue')
        if 'angular' in html.lower() or 'ng-app' in html:
            frameworks.append('Angular')

        analysis['structure']['javascript_frameworks'] = frameworks

        # ç©ºçŠ¶æ…‹ã®æ¤œå‡º
        empty_keywords = [
            'ç¾åœ¨ã€è­²æ¸¡å¯èƒ½ãªçŒ«ã¯ã„ã¾ã›ã‚“',
            'è­²æ¸¡å¯¾è±¡ã®çŒ«ã¯ã„ã¾ã›ã‚“',
            'æ²è¼‰ã•ã‚Œã¦ã„ã‚‹çŒ«ã¯ã„ã¾ã›ã‚“',
            'å‹Ÿé›†ä¸­ã®çŒ«ã¯ã„ã¾ã›ã‚“'
        ]

        page_text = soup.get_text()
        found_empty_keywords = [kw for kw in empty_keywords if kw in page_text]
        analysis['structure']['empty_state_keywords'] = found_empty_keywords

        print(f"  âœ… Analysis complete")
        print(f"     - Tables: {len(analysis['structure']['tables'])}")
        print(f"     - Images: {analysis['structure']['images']['total']}")
        print(f"     - JS Frameworks: {frameworks if frameworks else 'None'}")
        print(f"     - Possible containers: {len(possible_containers)}")

        return {'success': True, 'analysis': analysis}

    except Exception as e:
        print(f"  âŒ Error: {str(e)[:100]}")
        return {'success': False, 'error': str(e)}

async def investigate_sites():
    """å…¨ã‚µã‚¤ãƒˆã‚’èª¿æŸ»"""
    results = []

    async with async_playwright() as p:
        # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
        print("ğŸš€ Launching browser...")
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        page = await context.new_page()

        for site in SITES_TO_INVESTIGATE:
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ {site['name']} ({site['prefecture']})")
            print(f"   {site['url']}")
            print('='*60)

            result = await analyze_page_structure(page, site['url'])
            result.update({
                'name': site['name'],
                'prefecture': site['prefecture'],
                'region': site['region'],
                'url': site['url']
            })
            results.append(result)

            # ç¤¼å„€æ­£ã—ãå¾…æ©Ÿ
            await asyncio.sleep(4)

        await browser.close()

    return results

async def main():
    print("="*60)
    print("ğŸ± Playwright Site Structure Investigation")
    print("="*60)

    results = await investigate_sites()

    # JSONä¿å­˜
    output_file = 'playwright_investigation_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print(f"âœ… Investigation complete")
    print(f"   Results saved to: {output_file}")
    print('='*60)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    success_count = sum(1 for r in results if r.get('success'))
    print(f"\nğŸ“Š Summary:")
    print(f"   Investigated: {len(results)} sites")
    print(f"   Successful: {success_count}")
    print(f"   Failed: {len(results) - success_count}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
    js_sites = [r for r in results if r.get('success') and r['analysis']['structure'].get('javascript_frameworks')]
    if js_sites:
        print(f"\nğŸ”§ JavaScript Frameworks detected:")
        for site in js_sites:
            print(f"   - {site['prefecture']}: {site['analysis']['structure']['javascript_frameworks']}")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ãƒˆ
    table_sites = [r for r in results if r.get('success') and r['analysis']['structure'].get('tables')]
    if table_sites:
        print(f"\nğŸ“Š Table-based sites:")
        for site in table_sites:
            table_count = len(site['analysis']['structure']['tables'])
            print(f"   - {site['prefecture']}: {table_count} tables")

if __name__ == '__main__':
    asyncio.run(main())
