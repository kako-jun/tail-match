"""
Universal Scraper for All Municipalities
å…¨å›½è‡ªæ²»ä½“å¯¾å¿œæ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import logging
from typing import Dict, List, Any
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from scraper_base import BaseScraper
from local_extractor import local_extractor
from playwright.sync_api import sync_playwright
import time

logger = logging.getLogger(__name__)

class UniversalScraper(BaseScraper):
    """å…¨å›½ã®è‡ªæ²»ä½“ã‚µã‚¤ãƒˆå¯¾å¿œæ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, municipality_id: int):
        super().__init__(municipality_id)
        
        # è‡ªæ²»ä½“ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‹•çš„å­¦ç¿’ã™ã‚‹ãŸã‚ã®è¨­å®š
        self.learned_patterns = {}
        self.extraction_strategies = [
            'standard_extraction',
            'aggressive_extraction', 
            'broken_html_extraction',
            'manual_content_extraction'
        ]
        
        logger.info(f"Initialized universal scraper for municipality {municipality_id}")

    def extract_tail_data(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """
        æ±ç”¨çŒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆé™çš„â†’å‹•çš„ã®2æ®µæ§‹ãˆï¼‰
        """
        logger.info(f"Starting universal extraction for: {base_url}")
        
        all_cats = []
        strategy_results = {}
        
        # æˆ¦ç•¥1: æ¨™æº–çš„ãªæ§‹é€ åŒ–æŠ½å‡º
        standard_cats = self._standard_extraction(soup, base_url)
        all_cats.extend(standard_cats)
        strategy_results['standard'] = len(standard_cats)
        logger.info(f"Standard extraction: {len(standard_cats)} cats")
        
        # æˆ¦ç•¥2: ç©æ¥µçš„æŠ½å‡ºï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        aggressive_cats = self._aggressive_extraction(soup, base_url)
        all_cats.extend(aggressive_cats)
        strategy_results['aggressive'] = len(aggressive_cats)
        logger.info(f"Aggressive extraction: {len(aggressive_cats)} cats")
        
        # æˆ¦ç•¥3: ç ´æHTMLå¯¾å¿œæŠ½å‡º
        broken_cats = self._broken_html_extraction(soup, base_url)
        all_cats.extend(broken_cats)
        strategy_results['broken_html'] = len(broken_cats)
        logger.info(f"Broken HTML extraction: {len(broken_cats)} cats")
        
        # æˆ¦ç•¥4: æ‰‹å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        manual_cats = self._manual_content_extraction(soup, base_url)
        all_cats.extend(manual_cats)
        strategy_results['manual'] = len(manual_cats)
        logger.info(f"Manual content extraction: {len(manual_cats)} cats")
        
        # æˆ¦ç•¥5: JavaScriptæ¤œå‡ºãƒ»å‹•çš„å‡¦ç† ğŸ†•
        js_detected = self._detect_javascript_content(soup)
        if js_detected and len(all_cats) == 0:
            logger.info("ğŸ” JavaScript content detected, no cats found - attempting dynamic extraction")
            dynamic_cats = self._dynamic_extraction(base_url)
            all_cats.extend(dynamic_cats)
            strategy_results['dynamic'] = len(dynamic_cats)
            logger.info(f"Dynamic extraction: {len(dynamic_cats)} cats")
        elif js_detected and len(all_cats) > 0:
            logger.info(f"ğŸ” JavaScript detected but {len(all_cats)} cats found - skipping dynamic extraction")
            strategy_results['dynamic'] = 0
        
        # é‡è¤‡é™¤å»
        unique_cats = self._deduplicate_universal_cats(all_cats)
        
        # çµæœã®åˆ†æã¨ãƒ­ã‚®ãƒ³ã‚°
        total_found = len(all_cats)
        unique_found = len(unique_cats)
        
        logger.info(f"Universal extraction completed:")
        logger.info(f"  Total extractions: {total_found}")
        logger.info(f"  Unique cats: {unique_found}")
        logger.info(f"  Strategy breakdown: {strategy_results}")
        
        # æˆ¦ç•¥ã®æœ‰åŠ¹æ€§ã‚’å­¦ç¿’
        self._learn_effective_strategies(strategy_results, base_url)
        
        # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
        if total_found > 0 and unique_found == 0:
            logger.error("ğŸš¨ CRITICAL: All cats filtered out during deduplication!")
            logger.error("    This could mean cats are being missed - manual review needed")
        
        if unique_found == 0 and any(strategy_results.values()):
            logger.warning("âš ï¸ No cats found despite positive strategy results - possible false negatives")
        
        return unique_cats

    def _standard_extraction(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """æ¨™æº–çš„ãªæ§‹é€ åŒ–æŠ½å‡º"""
        return local_extractor.extract_from_html(soup, base_url)

    def _aggressive_extraction(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """ç©æ¥µçš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æŠ½å‡º"""
        cats = []
        
        # ã‚ˆã‚Šåºƒç¯„å›²ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        extended_keywords = [
            'çŒ«', 'ãƒã‚³', 'ã­ã“', 'cat', 'å­çŒ«', 'ã“ã­ã“', 'ä»”çŒ«',
            'ä¿è­·', 'åå®¹', 'é‡Œè¦ª', 'è­²æ¸¡', 'é£¼ã„ä¸»', 'å‹•ç‰©', 'ãƒšãƒƒãƒˆ'
        ]
        
        for keyword in extended_keywords:
            elements = soup.find_all(text=lambda text: text and keyword in text)
            for text_node in elements[:5]:  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«æœ€å¤§5ä»¶
                parent = text_node.parent
                if parent and self._looks_like_cat_info(parent.get_text()):
                    cat_data = self._extract_from_suspicious_element(parent, base_url, keyword)
                    if cat_data:
                        cats.append(cat_data)
        
        return cats

    def _broken_html_extraction(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """ç ´æHTMLå¯¾å¿œæŠ½å‡º"""
        cats = []
        
        # HTMLãŒæ­£ã—ãé–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„å ´åˆã®å‡¦ç†
        html_str = str(soup)
        
        # ã‚ˆãã‚ã‚‹æ‰‹å‹•ç·¨é›†ã®ç—•è·¡ã‚’æ¤œå‡º
        manual_patterns = [
            r'çŒ«.*?(?=çŒ«|\Z)',  # "çŒ«"ã‹ã‚‰æ¬¡ã®"çŒ«"ã¾ã§ã€ã¾ãŸã¯æ–‡æœ«ã¾ã§
            r'åå‰.*?(?=åå‰|\Z)',  # "åå‰"ã‹ã‚‰æ¬¡ã®"åå‰"ã¾ã§ã€ã¾ãŸã¯æ–‡æœ«ã¾ã§
            r'æ€§åˆ¥.*?å¹´é½¢.*?è‰².*?',  # æ€§åˆ¥ã€å¹´é½¢ã€è‰²ãŒé€£ç¶šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        ]
        
        for pattern in manual_patterns:
            import re
            matches = re.findall(pattern, html_str, re.DOTALL | re.IGNORECASE)
            for i, match in enumerate(matches[:10], 1):
                if len(match) > 10 and len(match) < 500:  # é©åº¦ãªé•·ã•
                    cat_data = self._extract_from_text_block(match, base_url, i)
                    if cat_data:
                        cats.append(cat_data)
        
        return cats

    def _manual_content_extraction(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """æ‰‹å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰"""
        cats = []
        
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æ
        all_text = soup.get_text()
        
        # æ”¹è¡Œã‚„å¥èª­ç‚¹ã§åˆ†å‰²ã—ã¦ã€å„ãƒ–ãƒ­ãƒƒã‚¯ã‚’è©•ä¾¡
        text_blocks = []
        for separator in ['\n\n', 'ã€‚', 'ï¼', 'â€»', 'â˜…', 'â—']:
            if separator in all_text:
                text_blocks.extend(all_text.split(separator))
        
        # é‡è¤‡é™¤å»
        unique_blocks = list(set([block.strip() for block in text_blocks if len(block.strip()) > 20]))
        
        for i, block in enumerate(unique_blocks[:20], 1):  # æœ€å¤§20ãƒ–ãƒ­ãƒƒã‚¯
            if self._looks_like_cat_info(block):
                cat_data = self._extract_from_text_block(block, base_url, i + 5000)
                if cat_data:
                    cat_data['external_id'] = f'manual_extraction_{i:03d}'
                    cats.append(cat_data)
        
        return cats

    def _looks_like_cat_info(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒçŒ«æƒ…å ±ã‚‰ã—ã„ã‹ã‚’åˆ¤å®š"""
        text_lower = text.lower()
        
        # çŒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¿…é ˆãƒã‚§ãƒƒã‚¯
        has_cat_keyword = any(keyword in text_lower for keyword in ['çŒ«', 'ãƒã‚³', 'ã­ã“', 'cat'])
        if not has_cat_keyword:
            return False
        
        # è©³ç´°æƒ…å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        detail_keywords = ['æ€§åˆ¥', 'å¹´é½¢', 'è‰²', 'æ¯›è‰²', 'ã‚ªã‚¹', 'ãƒ¡ã‚¹', 'æ­³', 'ãƒ¶æœˆ', 'ç™½', 'é»’', 'èŒ¶']
        detail_score = sum(1 for keyword in detail_keywords if keyword in text)
        
        # æœŸé™ã‚„çŠ¶æ…‹ã«é–¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        status_keywords = ['æœŸé™', 'é‡Œè¦ª', 'è­²æ¸¡', 'ä¿è­·', 'åå®¹', 'å‹Ÿé›†']
        status_score = sum(1 for keyword in status_keywords if keyword in text)
        
        # æœ€ä½2ã¤ã®è©³ç´°æƒ…å ±ã¾ãŸã¯1ã¤ã®çŠ¶æ…‹æƒ…å ±ãŒå¿…è¦
        return detail_score >= 2 or status_score >= 1

    def _extract_from_suspicious_element(self, element, base_url: str, keyword: str) -> Dict[str, Any]:
        """ç–‘ã‚ã—ã„è¦ç´ ã‹ã‚‰çŒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            text = element.get_text()
            
            # åŸºæœ¬çš„ãªæƒ…å ±ã‚’æŠ½å‡º
            name = self._extract_name_from_text(text)
            gender = local_extractor._extract_gender(text)
            age = local_extractor._extract_age(text)
            color = local_extractor._extract_color(text)
            
            return {
                'external_id': f'suspicious_{keyword}_{hash(text) % 1000:03d}',
                'animal_type': 'cat',
                'name': name or f'ä¿è­·çŒ«_{keyword}',
                'breed': 'ãƒŸãƒƒã‚¯ã‚¹',
                'age_estimate': age,
                'gender': gender,
                'color': color,
                'size': 'medium',
                'protection_date': (datetime.now() - timedelta(days=7)).date(),
                'deadline_date': (datetime.now() + timedelta(days=21)).date(),
                'status': 'available',
                'transfer_decided': False,
                'source_url': base_url,
                'extraction_method': f'aggressive_{keyword}'
            }
        except Exception as e:
            logger.debug(f"Failed to extract from suspicious element: {e}")
            return None

    def _extract_from_text_block(self, text: str, base_url: str, index: int) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰çŒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            name = self._extract_name_from_text(text)
            gender = local_extractor._extract_gender(text)
            age = local_extractor._extract_age(text)
            color = local_extractor._extract_color(text)
            
            return {
                'external_id': f'text_block_{index:03d}',
                'animal_type': 'cat',
                'name': name or f'ä¿è­·çŒ«{index:03d}å·',
                'breed': 'ãƒŸãƒƒã‚¯ã‚¹',
                'age_estimate': age,
                'gender': gender,
                'color': color,
                'size': 'medium',
                'protection_date': (datetime.now() - timedelta(days=7)).date(),
                'deadline_date': (datetime.now() + timedelta(days=21)).date(),
                'status': 'available',
                'transfer_decided': False,
                'source_url': base_url,
                'extraction_method': 'text_block'
            }
        except Exception as e:
            logger.debug(f"Failed to extract from text block: {e}")
            return None

    def _extract_name_from_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åå‰ã‚’æŠ½å‡º"""
        # åå‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢
        name_patterns = [
            r'åå‰[ï¼š:\s]*([^\s\nã€‚ã€]+)',
            r'ãªã¾ãˆ[ï¼š:\s]*([^\s\nã€‚ã€]+)',
            r'çŒ«ã®åå‰[ï¼š:\s]*([^\s\nã€‚ã€]+)',
        ]
        
        for pattern in name_patterns:
            import re
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return None

    def _deduplicate_universal_cats(self, cats: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ±ç”¨çŒ«ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡é™¤å»ï¼ˆéå¸¸ã«ä¿å®ˆçš„ï¼‰"""
        if not cats:
            return []
        
        unique_cats = []
        seen_fingerprints = set()
        
        for cat in cats:
            # ã‚ˆã‚Šå¯›å®¹ãªãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆä½œæˆ
            name = (cat.get('name', '') or '').lower().strip()
            gender = (cat.get('gender', '') or '').lower()
            age = (cat.get('age_estimate', '') or '').lower()
            color = (cat.get('color', '') or '').lower()
            
            # éå¸¸ã«ç‰¹å¾´çš„ãªçµ„ã¿åˆã‚ã›ã®ã¿ã‚’é‡è¤‡ã¨ã¿ãªã™
            fingerprint = f"{name}_{gender}_{age}_{color}"
            
            # ç©ºã®æƒ…å ±ãŒå¤šã„å ´åˆã¯é‡è¤‡åˆ¤å®šã‚’ç·©å’Œ
            empty_fields = sum(1 for field in [name, gender, age, color] if not field)
            if empty_fields >= 3:
                # æƒ…å ±ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã€ã‚ˆã‚Šå³å¯†ãªæ¡ä»¶ã§é‡è¤‡åˆ¤å®š
                fingerprint = f"{name}_{hash(str(cat)) % 10000}"
            
            if fingerprint not in seen_fingerprints:
                unique_cats.append(cat)
                seen_fingerprints.add(fingerprint)
            else:
                logger.debug(f"Duplicate cat filtered: {fingerprint}")
        
        return unique_cats

    def _learn_effective_strategies(self, strategy_results: Dict[str, int], base_url: str):
        """æœ‰åŠ¹ãªæˆ¦ç•¥ã‚’å­¦ç¿’ï¼ˆå°†æ¥çš„ãªæ”¹å–„ã®ãŸã‚ï¼‰"""
        domain = base_url.split('/')[2] if '//' in base_url else base_url
        
        if domain not in self.learned_patterns:
            self.learned_patterns[domain] = {}
        
        # å„æˆ¦ç•¥ã®æœ‰åŠ¹æ€§ã‚’è¨˜éŒ²
        for strategy, count in strategy_results.items():
            if strategy not in self.learned_patterns[domain]:
                self.learned_patterns[domain][strategy] = []
            
            self.learned_patterns[domain][strategy].append(count)
            
            # æœ€æ–°10å›ã®çµæœã®ã¿ä¿æŒ
            if len(self.learned_patterns[domain][strategy]) > 10:
                self.learned_patterns[domain][strategy] = self.learned_patterns[domain][strategy][-10:]
        
        logger.debug(f"Learning updated for {domain}: {self.learned_patterns[domain]}")

    def _detect_javascript_content(self, soup: BeautifulSoup) -> bool:
        """JavaScriptå‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œå‡º"""
        # 1. å¤–éƒ¨JSãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        external_scripts = soup.find_all('script', src=True)
        if len(external_scripts) > 2:  # åŸºæœ¬çš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªä»¥ä¸Š
            return True
        
        # 2. AJAX/Fetch ã®ç—•è·¡
        all_scripts = soup.find_all('script')
        ajax_keywords = ['ajax', 'fetch', 'XMLHttpRequest', '$.get', '$.post', 'axios']
        
        for script in all_scripts:
            script_text = str(script).lower()
            if any(keyword.lower() in script_text for keyword in ajax_keywords):
                return True
        
        # 3. ç©ºã®ã‚³ãƒ³ãƒ†ãƒŠï¼ˆJSã§åŸ‹ã‚ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ï¼‰
        import re
        full_html = str(soup).lower()
        api_patterns = [r'/api/', r'\.json', r'/data/', r'ajax']
        
        if any(re.search(pattern, full_html) for pattern in api_patterns):
            return True
        
        # 4. å‹•ç‰©é–¢é€£ã®ç©ºè¦ç´ 
        empty_divs = soup.find_all('div', class_=True)
        for div in empty_divs:
            if not div.get_text(strip=True) and len(div.find_all()) == 0:
                classes = ' '.join(div.get('class', []))
                if any(word in classes.lower() for word in ['pet', 'animal', 'cat', 'list', 'data']):
                    return True
        
        return False

    def _dynamic_extraction(self, base_url: str) -> List[Dict[str, Any]]:
        """Playwrightã‚’ä½¿ã£ãŸå‹•çš„ã‚µã‚¤ãƒˆå‡¦ç†ï¼ˆãƒ—ãƒ­ã‚­ã‚·å¯¾å¿œï¼‰"""
        try:
            # Playwrightã§ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿
            from playwright.sync_api import sync_playwright
            import os
            
            with sync_playwright() as p:
                # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
                https_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
                http_proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
                
                browser_args = {"headless": True}
                
                # ãƒ—ãƒ­ã‚­ã‚·ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä½¿ç”¨
                if https_proxy:
                    browser_args["proxy"] = {"server": https_proxy}
                    logger.info(f"ãƒ—ãƒ­ã‚­ã‚·ä½¿ç”¨: {https_proxy}")
                elif http_proxy:
                    browser_args["proxy"] = {"server": http_proxy}
                    logger.info(f"ãƒ—ãƒ­ã‚­ã‚·ä½¿ç”¨: {http_proxy}")
                else:
                    logger.info("ãƒ—ãƒ­ã‚­ã‚·ãªã—ã§å‹•ä½œ")
                
                browser = p.chromium.launch(**browser_args)
                page = browser.new_page()
                
                # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿
                page.goto(base_url, wait_until='networkidle', timeout=30000)
                
                # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                page.wait_for_timeout(5000)  # 5ç§’å¾…æ©Ÿ
                
                # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®HTMLã‚’å–å¾—
                html_content = page.content()
                browser.close()
                
                logger.info(f"å‹•çš„HTMLå–å¾—å®Œäº†: {len(html_content)} chars")
                
                # æ–°ã—ã„Soupã§å†è§£æ
                from bs4 import BeautifulSoup
                dynamic_soup = BeautifulSoup(html_content, 'lxml')
                
                # æ¨™æº–çš„ãªæŠ½å‡ºã‚’å†å®Ÿè¡Œ
                return self._standard_extraction(dynamic_soup, base_url)
                
        except ImportError:
            logger.error("Playwright not installed - dynamic extraction skipped")
            return []
        except Exception as e:
            logger.error(f"Dynamic extraction failed: {e}")
            return []