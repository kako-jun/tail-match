#!/usr/bin/env python3
"""
Direct Ishikawa Prefecture Scraping Test
çŸ³å·çœŒä¿è­·ã‚»ãƒ³ã‚¿ãƒ¼ã®ç›´æ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆï¼ˆDBä¸è¦ï¼‰
"""

import requests
import time
import logging
from bs4 import BeautifulSoup
from local_extractor import local_extractor
from ishikawa_scraper import IshikawaScraper

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def test_url_direct(url: str, name: str):
    """URLã‚’ç›´æ¥ãƒ†ã‚¹ãƒˆï¼ˆDBæ¥ç¶šä¸è¦ï¼‰"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Testing: {name}")
    print(f"ğŸ”— URL: {url}")
    print(f"{'='*60}")
    
    try:
        # User-Agentã‚’è¨­å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; TailMatch/1.0; +https://tailmatch.jp/robots)'
        }
        
        print("ğŸ“¡ Fetching webpage...")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print(f"âœ… HTTP {response.status_code} - {len(response.text)} characters")
        
        # HTMLã‚’è§£æ
        print("ğŸ” Parsing HTML...")
        soup = BeautifulSoup(response.text, 'lxml')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ç¢ºèª
        title = soup.title.string if soup.title else "No title"
        print(f"ğŸ“„ Page title: {title}")
        
        # çŒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        text_content = soup.get_text().lower()
        cat_keywords = ['çŒ«', 'ãƒã‚³', 'ã­ã“', 'cat', 'å‹•ç‰©', 'ä¿è­·', 'è­²æ¸¡', 'é‡Œè¦ª']
        found_keywords = [kw for kw in cat_keywords if kw in text_content]
        
        print(f"ğŸ”¤ Found keywords: {found_keywords}")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚’è©¦è¡Œ
        print("ğŸ¤– Running local extraction...")
        extracted_cats = local_extractor.extract_from_html(soup, url)
        
        print(f"ğŸ± Extracted cats: {len(extracted_cats)}")
        
        # çµæœã®è©³ç´°ã‚’è¡¨ç¤º
        if extracted_cats:
            print("\nğŸ“‹ Extraction Results:")
            for i, cat in enumerate(extracted_cats, 1):
                print(f"  Cat #{i}:")
                print(f"    Name: {cat.get('name', 'N/A')}")
                print(f"    Gender: {cat.get('gender', 'N/A')}")
                print(f"    Age: {cat.get('age_estimate', 'N/A')}")
                print(f"    Color: {cat.get('color', 'N/A')}")
                print(f"    External ID: {cat.get('external_id', 'N/A')}")
                print(f"    Method: {cat.get('extraction_method', 'N/A')}")
                if cat.get('images'):
                    print(f"    Images: {len(cat['images'])} found")
                print()
        else:
            print("âš ï¸  No cats extracted")
            
            # HTMLã®æ§‹é€ ã‚’åˆ†æ
            print("\nğŸ” HTML Structure Analysis:")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            tables = soup.find_all('table')
            print(f"  Tables found: {len(tables)}")
            for i, table in enumerate(tables[:3], 1):
                table_text = table.get_text()[:100]
                print(f"    Table {i}: {table_text}...")
            
            # ã‚«ãƒ¼ãƒ‰çš„ãªæ§‹é€ ã‚’ãƒã‚§ãƒƒã‚¯
            divs_with_class = soup.find_all('div', class_=True)
            print(f"  Divs with classes: {len(divs_with_class)}")
            
            # ãƒªã‚¹ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            lists = soup.find_all(['ul', 'ol'])
            print(f"  Lists found: {len(lists)}")
            
            # çŒ«é–¢é€£ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ ã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            print("\nğŸ“ Sample text containing cat keywords:")
            elements_with_cats = soup.find_all(text=lambda text: text and any(kw in text.lower() for kw in ['çŒ«', 'ãƒã‚³', 'ã­ã“']))
            for i, text in enumerate(elements_with_cats[:3], 1):
                clean_text = ' '.join(text.strip().split())
                if len(clean_text) > 10:
                    print(f"    {i}: {clean_text[:100]}...")
        
        return {
            'success': True,
            'cats_found': len(extracted_cats),
            'title': title,
            'keywords_found': found_keywords,
            'cats': extracted_cats
        }
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Network error: {e}")
        return {'success': False, 'error': str(e)}
    except Exception as e:
        print(f"âŒ Processing error: {e}")
        return {'success': False, 'error': str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ± Tail Match - Ishikawa Prefecture Direct Scraping Test")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®URL
    test_urls = {
        'ã„ã—ã‹ã‚å‹•ç‰©æ„›è­·ã‚»ãƒ³ã‚¿ãƒ¼': 'https://aigo-ishikawa.jp/petadoption_list/',
        'é‡‘æ²¢å¸‚å‹•ç‰©æ„›è­·ç®¡ç†ã‚»ãƒ³ã‚¿ãƒ¼': 'https://www4.city.kanazawa.lg.jp/soshikikarasagasu/dobutsuaigokanricenter/gyomuannai/1/jouto_info/index.html'
    }
    
    results = {}
    
    for name, url in test_urls.items():
        results[name] = test_url_direct(url, name)
        
        # æ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§é–“éš”ã‚’ã‚ã‘ã‚‹ï¼ˆç¤¼å„€æ­£ã—ãï¼‰
        if len(results) < len(test_urls):
            print("â³ Waiting 5 seconds before next request...")
            time.sleep(5)
    
    # ç·åˆçµæœ
    print(f"\n{'='*60}")
    print("ğŸ“Š SUMMARY RESULTS")
    print(f"{'='*60}")
    
    total_cats = 0
    successful_sites = 0
    
    for name, result in results.items():
        status = "âœ… SUCCESS" if result['success'] else "âŒ FAILED"
        cats = result.get('cats_found', 0)
        total_cats += cats
        
        if result['success']:
            successful_sites += 1
        
        print(f"{name}: {status} - {cats} cats found")
        
        if not result['success']:
            print(f"  Error: {result.get('error', 'Unknown error')}")
    
    print(f"\nOverall:")
    print(f"  Sites tested: {len(test_urls)}")
    print(f"  Successful: {successful_sites}")
    print(f"  Total cats found: {total_cats}")
    
    if total_cats > 0:
        print("\nğŸ‰ SUCCESS! Found cats in Ishikawa Prefecture!")
    else:
        print("\nâš ï¸  No cats found. This could mean:")
        print("   1. No cats currently available for adoption")
        print("   2. Website structure needs specific handling")
        print("   3. Content is dynamically loaded (JavaScript)")

if __name__ == '__main__':
    main()