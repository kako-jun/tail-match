#!/usr/bin/env python3
"""
Detailed Extraction Analysis for Ishikawa Sites
çŸ³å·çœŒã‚µã‚¤ãƒˆã®è©³ç´°æŠ½å‡ºåˆ†æ
"""

import requests
import re
from bs4 import BeautifulSoup

def analyze_ishikawa_aigo():
    """ã„ã—ã‹ã‚å‹•ç‰©æ„›è­·ã‚»ãƒ³ã‚¿ãƒ¼ã®è©³ç´°åˆ†æ"""
    url = 'https://aigo-ishikawa.jp/petadoption_list/'
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; python-requests/2.31.0)'
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    soup = BeautifulSoup(response.text, 'lxml')
    
    print("ğŸ” Analyzing Ishikawa Animal Protection Center HTML structure...")
    
    # ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚„IDä»˜ãã®è¦ç´ ã‚’æ¢ã™
    print("\n1. Looking for specific animal containers...")
    
    # .data_boxes ã‚„ .data_box ã‚’æ¢ã™
    data_boxes = soup.select('.data_boxes')
    print(f"   Found .data_boxes: {len(data_boxes)}")
    
    data_box_items = soup.select('.data_box')
    print(f"   Found .data_box: {len(data_box_items)}")
    
    # æœ€åˆã®ã„ãã¤ã‹ã‚’è©³ç´°åˆ†æ
    for i, box in enumerate(data_box_items[:3], 1):
        print(f"\n   === Data Box {i} ===")
        print(f"   Classes: {box.get('class', [])}")
        
        # dl/dt/dd æ§‹é€ ã‚’ãƒã‚§ãƒƒã‚¯
        dls = box.find_all('dl')
        print(f"   DL elements: {len(dls)}")
        
        for dl in dls:
            dts = dl.find_all('dt')
            dds = dl.find_all('dd')
            print(f"   DT/DD pairs: {len(dts)}/{len(dds)}")
            
            for dt, dd in zip(dts, dds):
                dt_text = dt.get_text(strip=True)
                dd_text = dd.get_text(strip=True)
                print(f"     {dt_text}: {dd_text}")
        
        # ç”»åƒã‚’ãƒã‚§ãƒƒã‚¯
        images = box.find_all('img')
        print(f"   Images: {len(images)}")
        for img in images:
            src = img.get('src', '')
            alt = img.get('alt', '')
            print(f"     Image: {src} (alt: {alt})")
        
        print(f"   Raw text sample: {box.get_text()[:200]}...")
    
    # çŒ«å°‚ç”¨ã®ã‚¯ãƒ©ã‚¹ã‚’æ¢ã™
    print("\n2. Looking for cat-specific elements...")
    cat_elements = soup.select('[class*="cat"]')
    print(f"   Elements with 'cat' in class: {len(cat_elements)}")
    
    neko_elements = soup.find_all(class_=re.compile(r'.*çŒ«.*|.*ãƒã‚³.*|.*ã­ã“.*'))
    print(f"   Elements with Japanese 'cat' in class: {len(neko_elements)}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    print("\n3. Analyzing text patterns...")
    all_text = soup.get_text()
    
    # æ€§åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
    gender_matches = re.findall(r'(ã‚ªã‚¹|ãƒ¡ã‚¹|é›„|é›Œ|â™‚|â™€)', all_text)
    print(f"   Gender mentions: {len(set(gender_matches))} unique - {set(gender_matches)}")
    
    # å¹´é½¢ãƒ‘ã‚¿ãƒ¼ãƒ³  
    age_matches = re.findall(r'(ç”Ÿå¾Œ\d+[ãƒ¶ã‹]?æœˆ|ç´„?\d+æ­³|å­çŒ«|æˆçŒ«|ã‚·ãƒ‹ã‚¢)', all_text)
    print(f"   Age mentions: {len(age_matches)} - {age_matches[:10]}")
    
    # è‰²ãƒ‘ã‚¿ãƒ¼ãƒ³
    color_matches = re.findall(r'(ç™½|é»’|èŒ¶|ç°|ä¸‰æ¯›|ã¿ã‘|ã‚­ã‚¸|ã‚µãƒ“|èŒ¶ç™½|ç™½é»’)', all_text)
    print(f"   Color mentions: {len(color_matches)} - {set(color_matches)}")
    
    # ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
    number_matches = re.findall(r'(No\.\s*\d+|â„–\s*\d+|\d+ç•ª)', all_text)
    print(f"   Number mentions: {len(number_matches)} - {number_matches[:10]}")

def analyze_kanazawa_city():
    """é‡‘æ²¢å¸‚å‹•ç‰©æ„›è­·ç®¡ç†ã‚»ãƒ³ã‚¿ãƒ¼ã®è©³ç´°åˆ†æ"""
    url = 'https://www4.city.kanazawa.lg.jp/soshikikarasagasu/dobutsuaigokanricenter/gyomuannai/1/jouto_info/index.html'
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; python-requests/2.31.0)'
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    soup = BeautifulSoup(response.text, 'lxml')
    
    print("\nğŸ” Analyzing Kanazawa City HTML structure...")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’åˆ†æ
    print("\n1. Table analysis...")
    tables = soup.find_all('table')
    print(f"   Found tables: {len(tables)}")
    
    for i, table in enumerate(tables, 1):
        table_text = table.get_text().lower()
        if any(keyword in table_text for keyword in ['çŒ«', 'ãƒã‚³', 'ã­ã“', 'å‹•ç‰©']):
            print(f"\n   === Table {i} (contains animal keywords) ===")
            rows = table.find_all('tr')
            print(f"   Rows: {len(rows)}")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
            if rows:
                headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]
                print(f"   Headers: {headers}")
                
                # ãƒ‡ãƒ¼ã‚¿è¡Œã‚µãƒ³ãƒ—ãƒ«
                for j, row in enumerate(rows[1:3], 1):
                    cells = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]
                    print(f"   Row {j}: {cells}")
    
    # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œã‚’ãƒã‚§ãƒƒã‚¯
    print("\n2. Character encoding check...")
    title = soup.title.string if soup.title else "No title"
    print(f"   Page title (raw): {repr(title)}")
    
    # æ­£ã—ã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å†å–å¾—ã‚’è©¦è¡Œ
    response_utf8 = requests.get(url, headers=headers, timeout=30)
    response_utf8.encoding = 'utf-8'
    soup_utf8 = BeautifulSoup(response_utf8.text, 'lxml')
    title_utf8 = soup_utf8.title.string if soup_utf8.title else "No title"
    print(f"   Page title (UTF-8): {title_utf8}")

def main():
    print("ğŸ± Detailed HTML Structure Analysis for Ishikawa Prefecture")
    print("=" * 60)
    
    try:
        analyze_ishikawa_aigo()
    except Exception as e:
        print(f"âŒ Error analyzing Ishikawa Aigo: {e}")
    
    try:
        analyze_kanazawa_city()
    except Exception as e:
        print(f"âŒ Error analyzing Kanazawa City: {e}")

if __name__ == '__main__':
    main()