#!/usr/bin/env python3
"""
JavaScript Content Detection for Ishikawa Sites
JavaScriptã§èª­ã¿è¾¼ã¾ã‚Œã‚‹å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡º
"""

import requests
import re
from bs4 import BeautifulSoup

def detect_javascript_content(url: str, name: str):
    """JavaScriptã«ã‚ˆã‚‹å‹•çš„èª­ã¿è¾¼ã¿ã‚’æ¤œå‡º"""
    print(f"\n{'='*60}")
    print(f"ğŸ” JavaScript Content Detection: {name}")
    print(f"ğŸ”— URL: {url}")
    print(f"{'='*60}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TailMatch/1.0; +https://tailmatch.jp/robots)'
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    soup = BeautifulSoup(response.text, 'lxml')
    
    # 1. JavaScript ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    print("1. JavaScript files loaded:")
    scripts = soup.find_all('script', src=True)
    for script in scripts:
        src = script.get('src')
        print(f"   - {src}")
    
    # 2. ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ JavaScript ã‚’ç¢ºèª
    print(f"\n2. Inline JavaScript blocks: {len(soup.find_all('script', src=False))}")
    inline_scripts = soup.find_all('script', src=False)
    for i, script in enumerate(inline_scripts[:3], 1):
        script_content = script.string or ""
        if len(script_content) > 50:
            print(f"   Script {i} preview: {script_content[:100]}...")
    
    # 3. AJAX ã‚„ fetch ã®ç—•è·¡ã‚’æ¢ã™
    print("\n3. AJAX/Fetch indicators:")
    all_scripts = soup.find_all('script')
    ajax_keywords = ['ajax', 'fetch', 'XMLHttpRequest', '$.get', '$.post', 'axios']
    
    for keyword in ajax_keywords:
        found = False
        for script in all_scripts:
            script_text = str(script).lower()
            if keyword.lower() in script_text:
                print(f"   âœ… Found '{keyword}' in JavaScript")
                found = True
                break
        if not found:
            print(f"   âŒ '{keyword}' not found")
    
    # 4. API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¯èƒ½æ€§ã‚’æ¢ã™
    print("\n4. Potential API endpoints:")
    api_patterns = [
        r'/api/[^\s"\']+',
        r'/json[^\s"\']*',
        r'\.json[^\s"\']*',
        r'/data/[^\s"\']+',
        r'ajax[^\s"\']*'
    ]
    
    full_html = str(soup).lower()
    for pattern in api_patterns:
        matches = re.findall(pattern, full_html)
        if matches:
            unique_matches = list(set(matches))
            print(f"   Pattern '{pattern}': {unique_matches[:5]}")
    
    # 5. ãƒ‡ãƒ¼ã‚¿å±æ€§ã‚„IDã‚’ç¢ºèª
    print("\n5. Data attributes and IDs:")
    elements_with_data = soup.find_all(attrs={'data-url': True})
    print(f"   Elements with data-url: {len(elements_with_data)}")
    
    elements_with_ids = soup.find_all(id=True)
    relevant_ids = [elem.get('id') for elem in elements_with_ids if any(word in elem.get('id', '').lower() for word in ['pet', 'animal', 'cat', 'dog', 'list', 'data'])]
    print(f"   Relevant IDs: {relevant_ids}")
    
    # 6. ç©ºã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¢ã™
    print("\n6. Empty containers (likely filled by JS):")
    empty_divs = soup.find_all('div', class_=True)
    for div in empty_divs:
        div_text = div.get_text(strip=True)
        if not div_text and len(div.find_all()) == 0:  # ãƒ†ã‚­ã‚¹ãƒˆã‚‚å­è¦ç´ ã‚‚ãªã„
            classes = div.get('class', [])
            print(f"   Empty div with classes: {classes}")
    
    # 7. å‹•ç‰©é–¢é€£ã®ã‚¯ãƒ©ã‚¹åã‚„IDã‚’åºƒãæ¢ã™
    print("\n7. Animal-related class names and IDs:")
    all_elements = soup.find_all(['div', 'section', 'article', 'ul', 'li'])
    for element in all_elements[:20]:  # æœ€åˆã®20è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
        classes = element.get('class', [])
        element_id = element.get('id', '')
        
        # å‹•ç‰©é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        combined = ' '.join(classes) + ' ' + element_id
        if any(word in combined.lower() for word in ['pet', 'animal', 'cat', 'dog', 'adoption', 'adopt']):
            print(f"   Element: {element.name}, classes: {classes}, id: {element_id}")
            text_preview = element.get_text(strip=True)[:50]
            print(f"     Text preview: {text_preview}...")

def main():
    print("ğŸ” JavaScript Content Detection for Ishikawa Sites")
    print("=" * 60)
    
    test_sites = {
        'ã„ã—ã‹ã‚å‹•ç‰©æ„›è­·ã‚»ãƒ³ã‚¿ãƒ¼': 'https://aigo-ishikawa.jp/petadoption_list/',
        'é‡‘æ²¢å¸‚å‹•ç‰©æ„›è­·ç®¡ç†ã‚»ãƒ³ã‚¿ãƒ¼': 'https://www4.city.kanazawa.lg.jp/soshikikarasagasu/dobutsuaigokanricenter/gyomuannai/1/jouto_info/index.html'
    }
    
    for name, url in test_sites.items():
        try:
            detect_javascript_content(url, name)
        except Exception as e:
            print(f"âŒ Error analyzing {name}: {e}")

if __name__ == '__main__':
    main()